{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "firstenv",
   "display_name": "firstEnv",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/work/vajira/DL/singan-polyp-aug\n"
     ]
    }
   ],
   "source": [
    "%cd /work/vajira/DL/singan-polyp-aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import get_arguments\n",
    "import pathlib\n",
    "import glob\n",
    "import argparse\n",
    "import sys\n",
    "from PIL import Image\n",
    "from skimage import io as img\n",
    "\n",
    "from SinGAN import functions\n",
    "from SinGAN import manipulate\n",
    "\n",
    "#from .singan_polyp_aug_temp.SinGAN import functions\n",
    "#from .singan_polyp_aug_temp.SinGAN import manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs_path = \"/work/vajira/DL/sinGAN/SinGAN-aug/TrainedModels/cju2u73dj53oz0878486k8k4b/scale_factor=0.750000,alpha=10/Gs.pth\"\n",
    "noise_path = \"/work/vajira/DL/sinGAN/SinGAN-aug/TrainedModels/cju2u73dj53oz0878486k8k4b/scale_factor=0.750000,alpha=10/NoiseAmp.pth\"\n",
    "reals_path = \"/work/vajira/DL/sinGAN/SinGAN-aug/TrainedModels/cju2u73dj53oz0878486k8k4b/scale_factor=0.750000,alpha=10/reals.pth\"\n",
    "z_path = \"/work/vajira/DL/sinGAN/SinGAN-aug/TrainedModels/cju2u73dj53oz0878486k8k4b/scale_factor=0.750000,alpha=10/Zs.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random Seed:  4318\n"
     ]
    }
   ],
   "source": [
    "parser = get_arguments()\n",
    "parser.add_argument('--input_dir', help='input image dir', default='/work/vajira/DATA/hyper_kvasir/data_new/segmented_train_val/data/img_and_mask_together')\n",
    "parser.add_argument('--input_name', help='input image name', default=\"cju2u73dj53oz0878486k8k4b.png\")\n",
    "parser.add_argument('--mode', help='random_samples | random_samples_arbitrary_sizes', default='random_samples')\n",
    "# for random_samples:\n",
    "parser.add_argument('--gen_start_scale', type=int, help='generation start scale', default=8)\n",
    "# for random_samples_arbitrary_sizes:\n",
    "parser.add_argument('--scale_h', type=float, help='horizontal resize factor for random samples', default=1.5)\n",
    "parser.add_argument('--scale_v', type=float, help='vertical resize factor for random samples', default=1)\n",
    "opt = parser.parse_args(\"\")\n",
    "opt = functions.post_config(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.gen_start_scale = 0\n",
    "opt.nc_z = 4\n",
    "opt.nc_im = 4\n",
    "opt.device = torch.device(\"cpu\" if opt.not_cuda else \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = functions.post_config(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path):\n",
    "    x = img.imread(path)\n",
    "\n",
    "    print(\"X shape=\", x.shape)\n",
    "    if len(x.shape) == 2: # executee if imag is gray\n",
    "        x = color.gray2rgb(x)\n",
    "    print(\"x after imread===\", x.shape)\n",
    "    x = functions.np2torch(x,opt)\n",
    "    #print(\"x after np2torch===\", x.shape)\n",
    "    #print(\" x.shape[1]=\", x.shape[1])\n",
    "    if x.shape[1] == 4:\n",
    "        x = x[:,0:4,:,:]    \n",
    "    else:\n",
    "        x = x[:,0:3,:,:]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X shape= (529, 524, 4)\nx after imread=== (529, 524, 4)\n"
     ]
    }
   ],
   "source": [
    "real = read_image(\"/work/vajira/DATA/hyper_kvasir/data_new/segmented_train_val/data/img_and_mask_together/cju2u73dj53oz0878486k8k4b.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       "\n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "functions.adjust_scales2image(real, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Namespace(Dsteps=3, Gsteps=3, alpha=10, beta1=0.5, device=device(type='cuda', index=0), gamma=0.1, gen_start_scale=0, input_dir='/work/vajira/DATA/hyper_kvasir/data_new/segmented_train_val/data/img_and_mask_together', input_name='cju2u73dj53oz0878486k8k4b.png', ker_size=3, lambda_grad=0.1, lr_d=0.0005, lr_g=0.0005, manualSeed=4318, max_size=250, min_nfc=32, min_nfc_init=32, min_size=25, mode='random_samples', nc_im=4, nc_z=4, netD='', netG='', nfc=32, nfc_init=32, niter=2000, niter_init=2000, noise_amp=0.1, noise_amp_init=0.1, not_cuda=0, num_layer=5, num_scales=12, out='Output', out_='TrainedModels/cju2u73dj53oz0878486k8k4b/scale_factor=0.750000/', padd_size=0, scale1=0.4725897920604915, scale_factor=0.7749549932130969, scale_factor_init=0.75, scale_h=1.5, scale_v=1, stop_scale=9, stride=1)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = torch.load(Gs_path)\n",
    "reals = torch.load(reals_path)\n",
    "Zs = torch.load(z_path)\n",
    "NoiseAmp = torch.load(noise_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "len(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(Gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_s = functions.generate_in2coarsest(reals, 1, 1, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[-0.9166, -0.8533, -0.8701,  ..., -0.9487, -1.0462, -0.9339],\n",
       "          [-0.8807, -0.8813, -0.8674,  ..., -0.8740, -0.9302, -0.9890],\n",
       "          [-0.9042, -0.8986, -0.9026,  ..., -0.9078, -0.8853, -0.8908],\n",
       "          ...,\n",
       "          [-0.9600, -0.9363, -0.9221,  ..., -0.8668, -0.8497, -0.8173],\n",
       "          [-1.0106, -0.9762, -0.9651,  ..., -0.9042, -0.9264, -0.8945],\n",
       "          [-1.0235, -1.0198, -0.9779,  ..., -0.9236, -0.9256, -0.9675]],\n",
       "\n",
       "         [[-0.9774, -0.9631, -0.9107,  ..., -0.9897, -1.0670, -0.9739],\n",
       "          [-0.9506, -1.0010, -0.9588,  ..., -0.9609, -1.0322, -1.0523],\n",
       "          [-0.9386, -0.9854, -0.9928,  ..., -0.9698, -0.9936, -0.9883],\n",
       "          ...,\n",
       "          [-1.0153, -1.0065, -0.9793,  ..., -0.9235, -0.9703, -0.9306],\n",
       "          [-1.0799, -1.0564, -1.0257,  ..., -0.9639, -1.0236, -0.9661],\n",
       "          [-1.0833, -1.0815, -1.0430,  ..., -0.9806, -0.9971, -1.0000]],\n",
       "\n",
       "         [[-0.9792, -1.0054, -0.9976,  ..., -0.9969, -1.0803, -0.9862],\n",
       "          [-0.9450, -1.0136, -0.9912,  ..., -0.9756, -0.9848, -1.0263],\n",
       "          [-0.9444, -0.9958, -1.0118,  ..., -0.9761, -0.9920, -0.9954],\n",
       "          ...,\n",
       "          [-1.0281, -1.0411, -1.0518,  ..., -1.0075, -1.0329, -0.9736],\n",
       "          [-1.0326, -1.0221, -1.0137,  ..., -0.9760, -1.0408, -1.0358],\n",
       "          [-1.0530, -1.0361, -1.0109,  ..., -0.9963, -1.0120, -1.0507]],\n",
       "\n",
       "         [[-0.8890, -0.8423, -0.8604,  ..., -0.9818, -0.9930, -0.9336],\n",
       "          [-0.9107, -0.8587, -0.8404,  ..., -0.9828, -0.9570, -0.9016],\n",
       "          [-0.9466, -0.9193, -0.9066,  ..., -0.9629, -0.9317, -0.8807],\n",
       "          ...,\n",
       "          [-0.9910, -0.9438, -0.8983,  ..., -0.9532, -0.9320, -0.8954],\n",
       "          [-0.9910, -0.9594, -0.9211,  ..., -0.9658, -0.9473, -0.9290],\n",
       "          [-0.9947, -0.9686, -0.9246,  ..., -0.9631, -0.9558, -0.9388]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "manipulate.SinGAN_generate(Gs, Zs, reals, NoiseAmp, opt , gen_start_scale=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/vajira/DATA/sinGAN_polyps/test_output/RandomSamples/test_input_/gen_start_scale=1/0.png'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4c11908f8cd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/work/vajira/DATA/sinGAN_polyps/test_output/RandomSamples/test_input_/gen_start_scale=1/0.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/vajira/anaconda3/envs/pytorch-new/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/vajira/DATA/sinGAN_polyps/test_output/RandomSamples/test_input_/gen_start_scale=1/0.png'"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"/work/vajira/DATA/sinGAN_polyps/test_output/RandomSamples/test_input_/gen_start_scale=1/0.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}